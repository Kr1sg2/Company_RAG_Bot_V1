# app/retrieval.py
import json
"""
Enhanced retrieval with BM25 re-ranking and answer guardrails.
"""
import re, os, logging, json
from typing import List, Dict, Any, Optional, Tuple
import chromadb
from chromadb.config import Settings
try:
    from rank_bm25 import BM25Okapi
    BM25_AVAILABLE = True
except ImportError:
    BM25_AVAILABLE = False

# Remove global OpenAI import - will be imported lazily

logger = logging.getLogger(__name__)

class EnhancedRetriever:
    def __init__(self, chroma_path: str, collection_name: str = "lexa_documents"):
        self.chroma_path = chroma_path
        self.collection_name = collection_name
        self.embed_model = os.getenv("LEXA_EMBED_MODEL", "text-embedding-3-large")
        
        self.client = chromadb.PersistentClient(
            path=chroma_path,
            settings=Settings(anonymized_telemetry=False)
        )
        try:
            self.collection = self.client.get_collection(collection_name)
        except Exception:
            logger.warning(f"Collection {collection_name} not found, creating new one")
            self.collection = self.client.get_or_create_collection(collection_name)

        # Policy preferences (configurable)
        self.policy_keywords = ["handbook", "policy", "benefit", "hr", "procedure", "manual"]

    def get_query_embedding(self, query: str) -> Optional[List[float]]:
        """Generate embedding for search query using same model as indexer."""
        # Lazy import so the app doesn't crash if OpenAI isn't present
        try:
            import openai  # type: ignore
        except Exception:
            logger.warning("OpenAI SDK not installed; falling back to query_texts mode")
            return None
            
        try:
            client = openai.OpenAI()
            response = client.embeddings.create(
                model=self.embed_model,
                input=[query]
            )
            return response.data[0].embedding
        except Exception as e:
            logger.error(f"Failed to generate query embedding: {e}")
            return None

    def retrieve_with_rerank(self, query: str, top_k: int = 20, final_k: int = 3) -> List[Dict[str, Any]]:
        try:
            # Get query embedding using same model as indexer
            query_embedding = self.get_query_embedding(query)
            if query_embedding:
                results = self.collection.query(
                    query_embeddings=[query_embedding],
                    n_results=top_k,
                    include=['documents', 'metadatas', 'distances']
                )
            else:
                results = self.collection.query(
                    query_texts=[query],
                    n_results=top_k,
                    include=['documents', 'metadatas', 'distances']
                )
            if not results['documents'][0]:
                logger.info("No documents found in vector search")
                return []

            candidates = []
            documents = results['documents'][0]
            metadatas = results['metadatas'][0]
            distances = results['distances'][0]

            for i, (doc, metadata, distance) in enumerate(zip(documents, metadatas, distances)):
                try:
                    sim = max(0.0, min(1.0, 1.0 - float(distance)))
                except Exception:
                    sim = 0.0
                candidates.append({
                    'text': doc,
                    'metadata': metadata or {},
                    'vector_score': sim,
                    'vector_rank': i + 1
                })

            if BM25_AVAILABLE and len(candidates) > 1:
                candidates = self._apply_bm25_rerank(query, candidates)
            else:
                for c in candidates:
                    c['bm25_score'] = c['vector_score']
                    c['combined_score'] = c['vector_score']

            candidates = self._apply_policy_boost(candidates)
            candidates.sort(key=lambda x: x['combined_score'], reverse=True)
            return candidates[:final_k]

        except Exception as e:
            logger.error(f"Error in retrieval: {e}")
            return []

    def _apply_bm25_rerank(self, query: str, candidates: List[Dict]) -> List[Dict]:
        try:
            tokenized_docs = [re.findall(r'\b\w+\b', c['text'].lower()) for c in candidates]
            if not tokenized_docs:
                return candidates
            bm25 = BM25Okapi(tokenized_docs)
            query_tokens = re.findall(r'\b\w+\b', query.lower())
            bm25_scores = bm25.get_scores(query_tokens)
            max_bm25 = max(bm25_scores) if getattr(bm25_scores, "size", None) else 1.0
            for i, c in enumerate(candidates):
                c['bm25_score'] = (bm25_scores[i] / max_bm25) if max_bm25 > 0 else 0
                c['combined_score'] = (0.6 * c['vector_score'] + 0.4 * c['bm25_score'])
            return candidates
        except Exception as e:
            logger.error(f"BM25 re-ranking failed: {e}")
            for c in candidates:
                c['bm25_score'] = 0
                c['combined_score'] = c['vector_score']
            return candidates

    def _apply_policy_boost(self, candidates: List[Dict]) -> List[Dict]:
        for c in candidates:
            file_name = c['metadata'].get('file_name', '').lower()
            rel = c['metadata'].get('relative_path', '').lower()
            boost = 1.0
            for kw in self.policy_keywords:
                if kw in file_name or kw in rel:
                    boost = 1.2
                    break
            c['policy_boost'] = boost
            c['combined_score'] *= boost
        return candidates

    def check_numeric_consistency(self, query: str, candidates: List[Dict]) -> Tuple[bool, Optional[str]]:
        numeric_pattern = r'\b\d+(?:\.\d+)?\s*(?:days?|years?|months?|weeks?|hours?|%|percent|dollars?|\$)\b'
        if not re.search(r'\d', query):
            return True, None
        facts = []
        for c in candidates[:3]:
            text = c['text'].lower()
            numbers = re.findall(numeric_pattern, text, re.IGNORECASE)
            if numbers:
                facts.append({'numbers': numbers, 'file_name': c['metadata'].get('file_name', ''), 'policy_boost': c.get('policy_boost', 1.0)})
        if len(facts) < 2:
            return True, None
        all_nums = [n for f in facts for n in f['numbers']]
        if len(set(all_nums)) > 1:
            pol = [f for f in facts if f['policy_boost'] > 1.0]
            if pol:
                best = max(pol, key=lambda x: x['policy_boost'])
                return False, f"According to {best['file_name']}: {', '.join(best['numbers'])}"
        return True, None

    def format_answer_with_citations(self, query: str, candidates: List[Dict]) -> Dict[str, Any]:
        if not candidates:
            return {'answer': "I couldn't find relevant information in the documents.", 'sources': [], 'confidence': 0.0}
        is_consistent, authoritative = self.check_numeric_consistency(query, candidates)
        answer_parts, sources, seen = [], [], set()
        for c in candidates[:3]:
            md = c['metadata']; file_name = md.get('file_name', 'Unknown'); page = md.get('page', 1)
            key = f"{file_name}::{page}"
            if key in seen: continue
            seen.add(key)
            snippet = c['text'][:200].strip() + ("..." if len(c['text']) > 200 else "")
            answer_parts.append(snippet)
            sources.append({'file_name': file_name, 'page': page, 'confidence': c['combined_score']})
        answer = authoritative if authoritative else " ".join(answer_parts)
        labels = [f"{s['file_name']}, p. {s['page']}" if s['page'] > 1 else s['file_name'] for s in sources]
        citation_text = f" — {'; '.join(labels)}" if sources else ""
        return {'answer': answer + citation_text, 'sources': sources, 'confidence': candidates[0]['combined_score'], 'is_consistent': is_consistent}

def enhanced_search(query: str, chroma_path: Optional[str] = None) -> Dict[str, Any]:
    chroma_path = chroma_path or os.getenv("LEXA_CHROMA_PATH", "chroma_db")
    retriever = EnhancedRetriever(chroma_path)
    candidates = retriever.retrieve_with_rerank(query)
    return make_final_answer(retriever, query, candidates)
# === Synthesized answer helpers (appended) ===
def _get_default_system_prompt() -> str:
    return (
        "You are the company knowledge-base assistant. Provide accurate, useful answers "
        "using ONLY the provided context. Do not include page numbers, file names, "
        "citations, or any retrieval details in your answer. If the context is "
        "insufficient or conflicting, say so briefly and suggest what to look for next. "
        "Prefer short paragraphs or tight bullets. Be precise and avoid speculation."
    )

def _load_system_prompt() -> str:
    import json, logging, os
    try:
        with open('/etc/lexa/branding.json', 'r') as f:
            branding = json.load(f)
        sp = branding.get('system_prompt')
        if sp:
            return sp
        logging.getLogger(__name__).warning("branding.json present but missing 'system_prompt'; using default.")
        return _get_default_system_prompt()
    except FileNotFoundError:
        logging.getLogger(__name__).warning("/etc/lexa/branding.json not found; using default system prompt.")
        return _get_default_system_prompt()
    except Exception as e:
        logging.getLogger(__name__).warning(f"Failed to load branding.json ({type(e).__name__}): {e}; using default.")
        return _get_default_system_prompt()

def _synthesize_answer(query: str, candidates: list) -> str:
    # Build a clean answer from top retrieved chunks via OpenAI chat completions.
    if not candidates:
        return "I couldn't find relevant information in the documents to answer your question."
    try:
        import openai, os, logging
        client = openai.OpenAI()
        # use top 3 chunks; truncate to keep prompt size reasonable
        parts = []
        for i, c in enumerate(candidates[:3]):
            txt = c.get("text", "") or ""
            if len(txt) > 800:
                txt = txt[:800] + "..."
            parts.append(f"Context {i+1}:\n{txt}")
        context = "\n\n".join(parts)

        system_prompt = _load_system_prompt()
        model = os.getenv("LEXA_CHAT_MODEL", "gpt-3.5-turbo")

        logging.getLogger(__name__).info(f"Calling OpenAI chat completions (model={model})")
        resp = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content":
                    "Answer the user's question using ONLY the provided context. "
                    "Do not include page numbers, file names, citations, or mention retrieval.\n\n"
                    f"Query: {query}\n\n{context}"
                },
            ],
            temperature=0.3,
            max_tokens=700,
        )
        return (resp.choices[0].message.content or "").strip()
    except Exception as e:
        import logging
        logging.getLogger(__name__).error(f"LLM synthesis failed ({type(e).__name__}): {str(e)[:200]}")
        return "I found related documents but couldn't synthesize a reliable answer. Please retry or refine the question."

def format_answer_synthesized(retriever, query: str, candidates: list) -> dict:
    # Keep internal shape: {"answer": ..., "sources": ...}
    if not candidates:
        return {'answer': "I couldn't find relevant information in the documents.", 'sources': [], 'confidence': 0.0}
    # numeric check (re-uses existing method on the retriever instance, if present)
    try:
        is_consistent, _ = retriever.check_numeric_consistency(query, candidates)
    except Exception:
        is_consistent = True

    # cap & dedupe sources by (file_name, page)
    import os
    max_sources = int(os.getenv("MAX_SOURCES", "3"))
    sources, seen = [], set()
    for c in candidates:
        md = c.get('metadata', {}) or {}
        fn = md.get('file_name', 'Unknown')
        page = md.get('page', 1)
        key = f"{fn}::{page}"
        if key in seen:
            continue
        seen.add(key)
        sources.append({'file_name': fn, 'page': page, 'confidence': c.get('combined_score', 0)})
        if len(sources) >= max_sources:
            break

    synthesized = _synthesize_answer(query, candidates)
    return {
        'answer': synthesized,
        'sources': sources,
        'confidence': candidates[0].get('combined_score', 0.0),
        'is_consistent': is_consistent
    }

# --- refine: treat "no answer" as no sources; filter sources by score ---
def _looks_insufficient(answer: str) -> bool:
    s = (answer or "").lower()
    flags = [
        "couldn't find", "could not find",
        "insufficient", "no relevant information",
        "couldn’t synthesize", "couldn't synthesize",
        "could not synthesize"
    ]
    return any(f in s for f in flags)

def format_answer_synthesized(retriever, query: str, candidates: list) -> dict:  # override earlier definition
    if not candidates:
        return {
            'answer': "I couldn't find relevant information in the documents.",
            'sources': [],
            'confidence': 0.0,
            'is_consistent': True
        }

    try:
        is_consistent, _ = retriever.check_numeric_consistency(query, candidates)
    except Exception:
        is_consistent = True

    synthesized = _synthesize_answer(query, candidates)

    # Threshold & cap for sources
    import os
    try:
        min_score = float(os.getenv("MIN_SOURCE_SCORE", "0.22"))
    except Exception:
        min_score = 0.22

    max_sources = int(os.getenv("MAX_SOURCES", "3"))
    sources, seen = [], set()
    for c in candidates:
        score = c.get('combined_score', 0.0)
        if score < min_score:
            continue
        md = c.get('metadata', {}) or {}
        fn = md.get('file_name', 'Unknown')
        page = md.get('page', 1)
        key = f"{fn}::{page}"
        if key in seen:
            continue
        seen.add(key)
        sources.append({'file_name': fn, 'page': page, 'confidence': score})
        if len(sources) >= max_sources:
            break

    # If the LLM says there isn't enough context, suppress sources entirely
    if _looks_insufficient(synthesized):
        sources = []

    return {
        'answer': synthesized,
        'sources': sources,
        'confidence': candidates[0].get('combined_score', 0.0),
        'is_consistent': is_consistent
    }

# --- refine (v2): stronger 'insufficient' detection & source suppression ---
import re as _re

def _looks_insufficient(answer: str) -> bool:  # override
    if not answer:
        return True
    s = _re.sub(r'\s+', ' ', answer).lower()
    patterns = [
        r"\binsufficient context\b",
        r"\bno (relevant )?information\b",
        r"\bdoes not contain (any )?(information|info)\b",
        r"\bnot found in the (provided )?context\b",
        r"\b(could not|couldn't|did not|didn't)\s(find|locate)\b",
        r"\bcontext (?:provided )?(?:lacks|does not have|has no)\b",
        r"\boutside (?:of )?scope\b",
        r"\bnot enough (?:context|information|info)\b",
    ]
    return any(_re.search(p, s) for p in patterns)

def format_answer_synthesized(retriever, query: str, candidates: list) -> dict:  # override
    if not candidates:
        return {
            'answer': "I couldn't find relevant information in the documents.",
            'sources': [],
            'confidence': 0.0,
            'is_consistent': True
        }

    try:
        is_consistent, _ = retriever.check_numeric_consistency(query, candidates)
    except Exception:
        is_consistent = True

    # Use the synthesizer already defined in this module
    try:
        synthesized = _synthesize_answer(query, candidates)
    except NameError:
        # If earlier helper name differs in your tree, fall back to retriever method
        synthesized = getattr(retriever, "synthesize_answer")(query, candidates)

    # Threshold & cap for sources
    import os
    try:
        min_score = float(os.getenv("MIN_SOURCE_SCORE", "0.22"))
    except Exception:
        min_score = 0.22
    max_sources = int(os.getenv("MAX_SOURCES", "3"))

    # Build filtered, deduped sources
    sources, seen = [], set()
    for c in candidates:
        score = c.get('combined_score', 0.0)
        if score < min_score:
            continue
        md = c.get('metadata', {}) or {}
        fn = md.get('file_name', 'Unknown')
        page = md.get('page', 1)
        key = f"{fn}::{page}"
        if key in seen:
            continue
        seen.add(key)
        sources.append({'file_name': fn, 'page': page, 'confidence': score})
        if len(sources) >= max_sources:
            break

    # If the LLM indicates insufficient context → suppress all sources
    if _looks_insufficient(synthesized):
        sources = []

    return {
        'answer': synthesized,
        'sources': sources,
        'confidence': candidates[0].get('combined_score', 0.0),
        'is_consistent': is_consistent
    }

# =========================
# === Synthesized output ===
# =========================

_INSUFF_PATTERNS = [
    r"\binsufficient context\b",
    r"\bnot (?:provided|found) in the context\b",
    r"\bno (?:relevant )?information\b",
    r"\bcontext (?:does not|doesn['’]t) contain\b",
    r"\bunavailable in the provided context\b",
    r"\bI (?:can|could) not find\b",
]

def _looks_insufficient(text: str) -> bool:
    if not text:
        return True
    tl = text.strip().lower()
    import re
    return any(re.search(p, tl) for p in _INSUFF_PATTERNS)

def _load_system_prompt() -> str:
    """
    Load a system prompt from /etc/lexa/branding.json if present;
    otherwise fall back to a safe default.
    """
    try:
        with open('/etc/lexa/branding.json', 'r') as f:
            data = json.load(f)
            sp = data.get('system_prompt')
            if sp:
                return sp
            logging.getLogger(__name__).warning(
                "branding.json present but missing 'system_prompt'; using default")
    except FileNotFoundError:
        logging.getLogger(__name__).warning(
            "/etc/lexa/branding.json not found; using default system prompt")
    except Exception as e:
        logging.getLogger(__name__).warning(
            f"Failed to load /etc/lexa/branding.json ({type(e).__name__}): {e}; using default")

    return (
        "You are the company knowledge-base assistant. Provide accurate, useful "
        "answers using ONLY the provided context. Do not include page numbers, "
        "file names, citations, or any retrieval details in your answer. If the "
        "context is insufficient or conflicting, say so briefly and suggest what "
        "to look for next. Prefer short paragraphs or tight bullets. Be precise "
        "and avoid speculation."
    )

def _build_context(candidates, max_docs: int = 3, per_chunk_limit: int = 900) -> str:
    parts = []
    for i, c in enumerate(candidates[:max_docs]):
        t = (c.get('text') or '').strip()
        if not t:
            continue
        if len(t) > per_chunk_limit:
            t = t[:per_chunk_limit] + "..."
        parts.append(f"Context {i+1}:\n{t}")
    return "\n\n".join(parts) if parts else ""

def _dedupe_and_filter_sources(candidates, max_sources: int, min_score: float):
    seen = set()
    out = []
    for c in candidates:
        md = c.get('metadata') or {}
        fn = md.get('file_name') or "Unknown"
        pg = md.get('page') or 1
        score = float(c.get('combined_score') or 0.0)
        key = (fn, pg)
        if key in seen:
            continue
        if score < min_score:
            continue
        seen.add(key)
        out.append({'file_name': fn, 'page': pg, 'confidence': score})
        if len(out) >= max_sources:
            break
    return out

def make_final_answer(retriever, query: str, candidates):
    """
    Single authoritative path:
    - Synthesize answer via OpenAI chat completion using top context.
    - If answer looks insufficient -> return sources: []
    - Else -> return deduped, gated, capped sources.
    """
    logger = logging.getLogger(__name__)
    if not candidates:
        return {'answer': "I couldn't find relevant information in the documents.",
                'sources': [], 'confidence': 0.0}

    # Build LLM inputs
    system_prompt = _load_system_prompt()
    model = os.getenv("LEXA_CHAT_MODEL", "gpt-3.5-turbo")
    context = _build_context(candidates, max_docs=3, per_chunk_limit=900)
    user_msg = f"Query: {query}\n\nContext:\n{context}\n\n" \
               "Instructions:\n" \
               "- Answer ONLY from the context.\n" \
               "- Do NOT mention files, page numbers, or retrieval.\n" \
               "- If context is insufficient, say so briefly.\n"

    # Call OpenAI
    try:
        import openai
        client = openai.OpenAI()
        logger.info(f"Calling OpenAI chat completion (model={model})")
        resp = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_msg},
            ],
            temperature=0.3,
            max_tokens=700,
        )
        answer = (resp.choices[0].message.content or "").strip()
    except Exception as e:
        logger.error(f"LLM synthesis failed ({type(e).__name__}): {e}")
        return {
            'answer': ("I found related documents but couldn’t synthesize a reliable answer. "
                       "Please retry or refine the question."),
            'sources': [],
            'confidence': float(candidates[0].get('combined_score') or 0.0)
        }

    # If answer is basically “no info”, suppress sources entirely
    if _looks_insufficient(answer):
        return {
            'answer': answer,
            'sources': [],
            'confidence': float(candidates[0].get('combined_score') or 0.0)
        }

    # Otherwise, keep only good, unique, top sources
    max_sources = int(os.getenv("MAX_SOURCES", "3"))
    min_score = float(os.getenv("MIN_SOURCE_SCORE", "0.25"))
    sources = _dedupe_and_filter_sources(candidates, max_sources=max_sources, min_score=min_score)

    return {'answer': answer, 'sources': sources,
            'confidence': float(candidates[0].get('combined_score') or 0.0)}
